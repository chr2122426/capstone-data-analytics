# -*- coding: utf-8 -*-
"""housingDataClean.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pnYNHoLjdrRD-Sa0Hj6_Go4boIC6iBu2
"""

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, year, explode, struct, lit, median, when
from pyspark.sql.types import DoubleType

# Initialize Spark Session (if not already active)
try:
    spark
except NameError:
    spark = SparkSession.builder.appName("HousingDataProcessor").getOrCreate()

# Upload data
from google.colab import files
uploaded = files.upload()

uploaded = files.upload()

# Load the Housing Data
housing_df = spark.read.csv("HousingData.csv", header=True, inferSchema=True)
housing_df.show(5)

housing_df = housing_df.drop("RegionID", "State", "CountyName", "SizeRank","RegionType","StateName","City","Metro")
housing_df.show(5)

housing_df =  housing_df.withColumnRenamed("RegionName", "ZIP")
housing_df.printSchema()

import pandas as pd

pandas_hpi_df = pd.read_excel("hpi_at_zip5.xlsx")
hpi_df = spark.createDataFrame(pandas_hpi_df)

hpi_df.show(5)

renamed_hpi_df = hpi_df.withColumnRenamed("Five-Digit ZIP Code", "ZIP")
renamed_hpi_df.printSchema()

filtered_hpi_df = renamed_hpi_df.filter(hpi_df.Year.between(2000, 2023))

filtered_hpi_df.show(5)

from pyspark.sql import functions as F

# Identify all date columns
date_cols = [c for c in housing_df.columns if c != "ZIP"]

# Build the stack() expression
# stack(n, col_name1, col_value1, ...) returns two columns (date, value)
expr = "stack({}, {}) as (Date, Value)".format(
    len(date_cols),
    ", ".join([f"'{c}', `{c}`" for c in date_cols])
)

# Apply the transformation
housing_df_unpivoted = housing_df.select("ZIP", F.expr(expr))

# Convert Date strings into actual dates
housing_df_unpivoted = housing_df_unpivoted.withColumn("Date", F.to_date("Date"))

housing_df_unpivoted.show(5)

housing_df_unpivoted.orderBy("ZIP").show(10)

# Ensure Year column exits
housing_df_with_year = housing_df_unpivoted.withColumn("Year", F.year("Date"))

# Compute the median (50th percentile) for each year
housing_df_median_yearly = (
    housing_df_with_year
    .groupBy("ZIP", "Year")
    .agg(F.expr("percentile_approx(Value, 0.5)").alias("Median_Value"))
    .orderBy("ZIP", "Year")
)

housing_df_median_yearly.show()

filtered_hpi_matched = (
    filtered_hpi_df.join(housing_df_median_yearly, on=["ZIP", "Year"], how="inner")
)

filtered_hpi_matched.printSchema()
filtered_hpi_matched.show()

output_path = "housingDataComplete"
filtered_hpi_matched.coalesce(1).write.csv(
    path=output_path,
    header=True,
    mode="overwrite"
)