# -*- coding: utf-8 -*-
"""LightRailData.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_8WXjP3qzHvb__Msyee-FtpwokT2-hjx
"""

# set Java environment
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!pip install pyspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"

# Import statemants
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, min, max, count, round, lit, when
from pyspark.sql.types import IntegerType

try:
    spark.stop()
except:
    pass # Ignore if 'spark' is not defined
spark = SparkSession.builder.appName("LightRailData").getOrCreate()

sc = spark.sparkContext

# Upload data
from google.colab import files
uploaded = files.upload()

df = spark.read.csv("ValleyMetroRailStations_-7532295976615093627.csv", header=True, inferSchema=True)

df.printSchema()
df.show(5)

df_clean = df.select(
    col("OBJECTID"),
    col("StationName"),
    col("POINT_X"),
    col("POINT_Y"),
    col("Address"),
    col("x"),
    col("y"))

df_clean.printSchema()
df_clean.show(5)

# Save the updated DataFrame as a CSV file
output_csv_path_updated = "updated_lightRail.csv"
df_clean.write.mode("overwrite").csv(output_csv_path_updated, header=True)
print(f"\nUpdated records saved to '{output_csv_path_updated}'.")

df_2 = spark.read.csv("geonames-postal-code.csv", header=True, inferSchema=True)

df_2.printSchema()
df_2.show(5)

df_clean2 = df_2.select(
    col("postal code"),
    col("latitude"),
    col("longitude"),
    col("accuracy"))

df_clean2.show()

# Save the updated DataFrame as a CSV file
output_csv_path_updated = "updated_ZIp.csv"
df_clean2.write.mode("overwrite").csv(output_csv_path_updated, header=True)
print(f"\nUpdated records saved to '{output_csv_path_updated}'.")

uploaded = files.upload()

import numpy as np
from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import col, udf, lit, row_number, when
from pyspark.sql.types import DoubleType

def haversine(lon1, lat1, lon2, lat2):
    """Calculates the distance (miles) between two Lon/Lat points."""
    R = 3958.8  # Radius of Earth in miles
    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])

    dlon = lon2 - lon1
    dlat = lat2 - lat1

    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2
    c = 2 * np.arcsin(np.sqrt(a))
    distance = R * c
    return float(distance) # Ensure the return type is a native float

# Register the UDF with Spark
haversine_udf = udf(haversine, DoubleType())
spark.udf.register("haversine_udf", haversine, DoubleType())

zip_path = "updated_ZIp.csv"
zip_df = spark.read.csv(zip_path, header=True, inferSchema=True) \
    .select(
        col("postal code").alias("ZIP"),
        col("latitude").alias("ZIP_Lat"),
        col("longitude").alias("ZIP_Lon")
    )

# Load Light Rail Stations
rail_path = "updated_lightRail.csv"
rail_df = spark.read.csv(rail_path, header=True, inferSchema=True) \
    .select(
        col("OBJECTID").alias("StationID"),
        col("StationName"),
        col("POINT_X").alias("Station_Lon"),
        col("POINT_Y").alias("Station_Lat")
    )

# Cross Join combines every row from zip_df with every row from rail_df
cross_df = zip_df.crossJoin(rail_df)

# Calculate the distance in miles using the Haversine UDF
distance_df = cross_df.withColumn(
    "Distance_Miles",
    haversine_udf(
        col("ZIP_Lon"), col("ZIP_Lat"),
        col("Station_Lon"), col("Station_Lat")
    )
)

# Define a window partitioned by ZIP, ordered by distance
window_spec = Window.partitionBy("ZIP").orderBy(col("Distance_Miles").asc())

# Rank the stations by distance for each ZIP code
ranked_df = distance_df.withColumn("rank", row_number().over(window_spec))

# Filter to keep only the nearest station (rank = 1)
nearest_station_df = ranked_df.filter(col("rank") == 1)

ADJACENCY_THRESHOLD_MILES = 2.5

final_proximity_df = nearest_station_df.select(
    "ZIP",
    col("Distance_Miles").alias("Rail_Distance_Miles"),
    "StationName",
    # Create the binary Adjacency Indicator
    when(col("Distance_Miles") <= ADJACENCY_THRESHOLD_MILES, 1)
        .otherwise(0)
        .alias("Adjacency_Indicator")
)

# Show the resulting feature table
print("\n--- Final Proximity Features Schema ---")
final_proximity_df.printSchema()
print("\n--- Final Proximity Features Head (Nearest Station per ZIP) ---")
final_proximity_df.show(5, truncate=False)

final_proximity_df.write.csv("zip_rail_proximity_features.csv", header=True, mode="overwrite")