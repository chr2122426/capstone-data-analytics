# -*- coding: utf-8 -*-
"""CountyBusinessPatterns_Details.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TpmiBx76OFm4TXftsXhstJTtVAqP5wy0

# County Business Patterns Data Aggregation 2000-23
"""

# Import statemants
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, min, max, count, round, lit, when
from pyspark.sql.types import IntegerType

# Create Spark environment
spark = SparkSession.builder.appName("CountyBusinessPatterns").getOrCreate()

sc = spark.sparkContext

"""### Load Data"""

# Upload data
from google.colab import files
uploaded = files.upload()

# Upload data
from google.colab import files
uploaded = files.upload()

"""### Data Processing"""

# Define File and Schema Mappings
file_map = {
    2000: "zbp00detail.txt", 2001: "zbp01detail.txt", 2002: "zbp02detail.txt",
    2003: "zbp03detail.txt", 2004: "zbp04detail.txt", 2005: "zbp05detail.txt",
    2006: "zbp06detail.txt", 2007: "zbp07detail.txt", 2008: "zbp08detail.txt",
    2009: "zbp09detail.txt", 2010: "zbp10detail.txt", 2011: "zbp11detail.txt",
    2012: "zbp12detail.txt", 2013: "zbp13detail.txt", 2014: "zbp14detail.txt",
    2015: "zbp15detail.txt", 2016: "zbp16detail.txt", 2017: "zbp17detail.txt",
    2018: "zbp18detail.txt", 2019: "zbp19detail.txt", 2020: "zbp20detail.txt",
    2021: "zbp21detail.txt", 2022: "zbp22detail.txt", 2023: "zbp23detail.txt"
}

# Define Standardized Columns to Keep
TARGET_COLUMNS = ["ZIP", "NAICS","EstablishmentCount","Year"]

"""#### Cleaning Functions to Handle Schema Differences

"""

from pyspark.sql.functions import col, lit, upper, substring

# Cleaning Function
def clean_df(file_name, year):

    df = spark.read.csv(f"{file_name}", header=True, inferSchema=True)

    # Standardize columns
    df = df.select(
        upper(col("zip")).alias("ZIP"),
        col("naics").alias("NAICS"),
        col("est").alias("EstablishmentCount"),
        lit(year).alias("Year")
    )
    return df

# List to hold cleaned DataFrames
cleaned_dfs = []

# read each file from file map
for year, file_name in file_map.items():
  cleaned_df = clean_df(file_name, year)

  cleaned_dfs.append(cleaned_df)

"""#### Data Cleaning"""

# Start with the first DataFrame in the list
cbp_panel_df = cleaned_dfs[0]

# Iteratively union the rest of the DataFrames
for next_df in cleaned_dfs[1:]:
    cbp_panel_df = cbp_panel_df.union(next_df)

# Read file for Zip code loaction data
postal_code_df = spark.read.csv("geonames-postal-code.csv", header=True, inferSchema=True)

# Standardize columns
postal_code_df = postal_code_df.select(
    col("postal code").alias("ZIP").cast("string"),
    col("latitude").alias("latitude"),
    col("longitude").alias("longitude")
).distinct()

#join data
az_zbp_panel_df = cbp_panel_df.join(
    postal_code_df,
    on="ZIP",
    how="inner"
)

# write to file
output_path = "Combined_CBPData_2000-23.csv"
az_zbp_panel_df.coalesce(1).write.csv(
    path=output_path,
    header=True,
    mode="overwrite"
)