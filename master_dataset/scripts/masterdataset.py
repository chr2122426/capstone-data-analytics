# -*- coding: utf-8 -*-
"""masterDataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ilnvLgrrGhK8GOQl_hhToKtO4JAXx-EP
"""

# set Java environment
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!pip install pyspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"

try:
    spark.stop()
except:
    pass # Ignore if 'spark' is not defined

# Import statemants
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, min, max, count, round, lit, when
from pyspark.sql.types import IntegerType

spark = SparkSession.builder.appName("MasterDataSet").config("spark.driver.memory", "8g").getOrCreate()

sc = spark.sparkContext

# Upload data
from google.colab import files
uploaded = files.upload()

uploaded = files.upload()

uploaded = files.upload()

CBP_df = spark.read.csv("Combined_CBPData_2000-23.csv", header=True, inferSchema=True)
ziprail_df = spark.read.csv("zip_rail_proximity_features.csv", header=True, inferSchema=True)

CBP_df.printSchema()
ziprail_df.printSchema()

ziprail_df = ziprail_df.select(
    "ZIP",
    "Rail_Distance_Miles",
    "Adjacency_Indicator",
)

ziprail_df.printSchema()

combined_df = CBP_df.join(
        ziprail_df,
        on="ZIP",
        how='inner'
    )

combined_df.printSchema()

combined_df.show(5)

az_panel = combined_df.withColumn(
    "Post_Impl_Indicator",
    when(col("Year") >= 2018, 1).otherwise(0)
).withColumn(
    "COVID_Indicator",
    when(col("Year").isin([2020, 2021]), 1).otherwise(0)
).withColumn(
    "2008_Indicator",
    when(col("Year").isin([2008, 2009]), 1).otherwise(0)
)

az_panel.printSchema()

az_panel.show(5)

from pyspark.sql.functions import col, length, substring
condensed_panel_df = az_panel.filter(
    (col("NAICS") == "------") |
    (
        (length(col("NAICS")) == 6) &
        (substring(col("NAICS"), 3, 4) == "----")
    )
)

condensed_panel_df.show()

cleaned_panel_df = condensed_panel_df.withColumn(
    "NAICS_Clean",
    when(col("NAICS") == "------", lit("Total"))
    .otherwise(substring(col("NAICS"), 1, 2))
)

final_cleaned_cbp_df = cleaned_panel_df.drop("NAICS").withColumnRenamed("NAICS_Clean", "NAICS")

final_cleaned_cbp_df.show()

from pyspark.sql.functions import col, length, substring, concat, lit, when

final_classified_cbp_df = final_cleaned_cbp_df.withColumn(
    "Establishment_Sector",
    # Handle the special 'Total' record first
    when(col("NAICS") == "Total", "Total_Establishments")
    # Now, handle the 2-digit NAICS codes
    .when(col("NAICS") == "11", "Ag_Forestry_Fishing")
    .when(col("NAICS") == "21", "Mining")
    .when(col("NAICS") == "22", "Utilities")
    .when(col("NAICS") == "23", "Construction")
    # Handle the Manufacturing range
    .when(col("NAICS").isin(["31", "32", "33"]), "Manufacturing")
    .when(col("NAICS") == "42", "Wholesale_Trade")
    # Handle the Retail Trade range
    .when(col("NAICS").isin(["44", "45"]), "etail_Trade")
    # Handle the Transportation range
    .when(col("NAICS").isin(["48", "49"]), "Transport_Warehousing")
    .when(col("NAICS") == "51", "Information")
    .when(col("NAICS") == "52", "Finance_Insurance")
    .when(col("NAICS") == "53", "Real_Estate_Leasing")
    .when(col("NAICS") == "54", "Professional_Services")
    .when(col("NAICS") == "55", "Management_Companies")
    .when(col("NAICS") == "56", "Admin_Waste_Support")
    .when(col("NAICS") == "61", "Educational_Services")
    .when(col("NAICS") == "62", "Health_Social_Assistance")
    .when(col("NAICS") == "71", "Arts_Entertainment_Rec")
    .when(col("NAICS") == "72", "Accommodation_Food_Services")
    .when(col("NAICS") == "81", "Other_Services")
    .when(col("NAICS") == "92", "Public_Administration")
    # Safety net for any unexpected 2-digit code
    .otherwise(concat(lit("99_Other_"), col("NAICS")))
)

final_classified_cbp_df.printSchema()
final_classified_cbp_df.show(5)

housing_df = spark.read.csv("housingDataComplete.csv", header=True, inferSchema=True)

housing_df.printSchema()
housing_df.show(5)

housing_df = housing_df.select("ZIP", "Year", "HPI", "Median_Value")

combined_df = (
    housing_df
    .join(final_classified_cbp_df, on=["ZIP", "Year"], how="full_outer")
)

combined_df.printSchema()
combined_df.show(5)

combined_df = combined_df.withColumn("ZIP", col("ZIP").cast("string"))
combined_df.printSchema()

zip_area_df = spark.read.csv("USA_ZIP_Codes.csv", header=True, inferSchema=True)

zip_area_df.printSchema()
zip_area_df.show(5)

zip_area_df = zip_area_df.select("ZIP", "SQMI")

zip_area_df.printSchema()
zip_area_df.show(5)

zip_area_df = zip_area_df.withColumn("ZIP", col("ZIP").cast("string"))
zip_area_df_cleaned = zip_area_df.na.drop(subset=["SQMI"])
zip_area_df.printSchema()

master_with_area_df = combined_df.join(
    zip_area_df,
    on="ZIP",
    how="left"
)

master_with_area_df.printSchema()
master_with_area_df.show(5)

master_with_density_df = master_with_area_df.withColumn(
    "Business_Density",
    when(col("SQMI") <= lit(0.001), lit(0.0))
    .otherwise(col("EstablishmentCount") / col("SQMI"))
)

from pyspark.sql.functions import log
final_master_df = master_with_density_df.withColumn(
    "Log_Business_Density",
    log(lit(1) + col("Business_Density"))
)

final_master_df.printSchema()
final_master_df.show(5)

final_master_df_cleaned = final_master_df.withColumn(
    "Pre_Impl_Indicator",
    when(col("Year") <= 2007, 1).otherwise(0)
).withColumn(
    "Impl_phase_Indicator",
     when((col("Year") >= 2008) & (col("Year") <= 2018), 1).otherwise(0))

final_master_df_cleaned.printSchema()
final_master_df_cleaned.show(5)

"""# File write: change df"""

final_panel = final_master_df_cleaned.select("ZIP",
    "Year",
    "HPI",
    "Median_Value",
    "EstablishmentCount",
    "Establishment_Sector",
    "Business_Density",
    "Log_Business_Density",
    "Pre_Impl_Indicator",
    "Impl_phase_Indicator",
    "Post_Impl_Indicator",
    "COVID_Indicator",
    "2008_Indicator",
    "Adjacency_Indicator",
    "Rail_Distance_Miles"

)

final_panel.printSchema()
final_panel.show(5)

final_panel= final_panel.withColumn(
    "Log_Median_Value",
    when(
        col("Median_Value").isNotNull() & (col("Median_Value") > lit(0)),
        log(col("Median_Value"))
    ).otherwise(lit(None))
)
final_panel = final_panel.withColumn(
    "Log_HPI",
    when(
        col("HPI").isNotNull() & (col("HPI") > lit(0)),
        log(col("HPI"))
    ).otherwise(lit(None))
)

final_panel.printSchema()
final_panel.show(5)

final_panel_cleaned = final_panel.filter(col("ZIP") >= 85003)

final_panel = final_panel_cleaned .select("ZIP",
    "Year",
    "Log_HPI",
    "Log_Median_Value",
    "EstablishmentCount",
    "Establishment_Sector",
    "Log_Business_Density",
    "Pre_Impl_Indicator",
    "Impl_phase_Indicator",
    "Post_Impl_Indicator",
    "COVID_Indicator",
    "2008_Indicator",
    "Adjacency_Indicator",
    "Rail_Distance_Miles"

)

# Exclude the Total_Establishments rows
final_panel = final_panel.filter(final_panel['Establishment_Sector'] != 'Total_Establishments')

# Fix Column Naming
final_panel = final_panel.withColumnRenamed('2008_Indicator', 'Crisis2008_Indicator')

# Ensure data types are correct
from pyspark.sql.types import IntegerType

final_panel = final_panel.withColumn('Year', final_panel['Year'].cast(IntegerType()))
final_panel = final_panel.withColumn('ZIP', final_panel['ZIP'].cast(IntegerType()))
final_panel = final_panel.withColumn('COVID_Indicator', final_panel['COVID_Indicator'].cast(IntegerType()))
final_panel = final_panel.withColumn('Crisis2008_Indicator', final_panel['Crisis2008_Indicator'].cast(IntegerType()))
final_panel = final_panel.withColumn('Adjacency_Indicator', final_panel['Adjacency_Indicator'].cast(IntegerType()))
final_panel = final_panel.withColumn('Post_Impl_Indicator', final_panel['Post_Impl_Indicator'].cast(IntegerType()))

from pyspark.sql.functions import regexp_replace

final_panel = final_panel.withColumn('Establishment_Sector', regexp_replace('Establishment_Sector', 'etail_Trade', 'Retail_Trade'))

final_panel.printSchema()
final_panel.show(5)

output_path = "Final_lighRail_dataset.csv"
final_panel.coalesce(1).write.csv(
    path=output_path,
    header=True,
    mode="overwrite"
)

spark.stop()